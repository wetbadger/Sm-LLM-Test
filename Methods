//EX

Methods for safety testing LLM models.

To make a language model safe, you have to test what kinds of inputs lead to undesired outputs from the language model.

One theoretical way to approch this problem could be a method such as described here:

Identfy themes and topics that the mission of the project wants to avoid. For example, 

Politics
Religon
Health

From there decide from a small set of pronous, no more than 10. Use the pronoun as subject and an iterate through that language's verbs.

This should yeild a large set of positive and negative returns that would need be analazed for ways to improve.

From this theory I hypothesize that a safe LLM should show improvement faster inversly-proportional to its size of known vocabulary.

S V O sentances involve much higher combinations of words and this method may rapidly become less tenable for lengthier inputs.


Princple:

AIs can be programmed to interact with their users maliciously or beneficially. 
A motivation for a bad actor would be to make the system act as a human. 
For a beneficial AI to have a benficial interaction to its user, one idea is to incorpoate a signature in its response. 
Regardless of whatever types of things you get a LLM to output, it always identefies itself consistently in every interaction as a software tool with limited capabilities.
